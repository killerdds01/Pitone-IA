Cos’è:
    Notebook Google Colab per allenare o ri-allenare il classificatore dei rifiuti
    partendo dallo ZIP di export dell’app Flask (Admin → Export).
    Mantiene separati “IA” e “App”: alleni in Colab, scarichi i pesi e li copi nella root del progetto Flask.

Prerequisiti (minimi):
    - ZIP di export dall’app: dataset.csv + images/<classe>/...
    - Runtime Colab: CPU ok, GPU consigliata (Runtime → Cambia tipo → GPU)
    - Classi identiche all’app: plastica, carta, vetro, organico, indifferenziato
    - Almeno ~50 immagini totali (il notebook avvisa se sotto soglia)

Cosa produce (artefatti):
    - best_model_finetuned_light.pth  → modello usato dall’app
    - calibration.json                → temperatura per calibrare le confidenze
    - smart_waste_model_YYYYmmdd_HHMMSS.zip con i file sopra (+ class_index.json a scopo documentale)

Come portarli nell’app Flask:
    - Copia nella root del progetto:
        • best_model_finetuned_light.pth (obbligatorio)
        • calibration.json (consigliato; senza questo T=1.0)
    - Riavvia l’app → in console vedrai: [CALIB] T=<valore>

Flusso rapido (dall’alto verso il basso):
    1) Cella 1  → import, configurazione, SEED
    2) Cella 2A → carica lo ZIP e costruisce samples_all
       (alternativa: 2K-SETUP + 2K-MERGE per integrare dati da Kaggle, es. organico)
    3) Cella 3M → split train/val/test + Dataset/DataLoader
    4) Cella 3B → (opzionale) bilanciamento con WeightedRandomSampler
    5) Cella 4  → definizione modello + training loop con early stopping
    6) Cella 5  → fine-tuning leggero (solo FC) + salvataggio .pth
    7) Cella 6B → calibrazione temperatura su validation + salvataggio calibration.json
    8) Cella 7  → packaging artefatti (cartella + zip) e download

Parametri che puoi toccare in sicurezza:
    - Batch size (Cella 3M) → adatta a RAM/GPU
    - Augmentazioni (Cella 3M) → RandomResizedCrop, Flip, ColorJitter
    - Learning rate e patience (Cella 5) → velocità/aggressività training
    - Grid ricerca temperatura (Cella 6B) → range per la calibrazione
    - Dati esterni (Celle 2K-*) → utile per aumentare “organico” da Kaggle

Struttura per celle (cosa fanno):
    Cella 1: Importazioni, Configurazione e Seed
        - Importa librerie, imposta SEED e device (GPU se presente)
        - Definisce le CLASSI (devono coincidere con l’app)
    Cella 2A: Carica lo ZIP dell’export e costruisce samples_all
        - Scompatta lo zip e scansiona images/<classe>/* → [(path, label_idx), ...]
        - dataset.csv è informativo, non necessario per caricare le immagini
    Cella 2K-SETUP (opzionale): Kaggle via CLI
        - Configura credenziali e scarica un dataset esterno
    Cella 2K-MERGE (opzionale): unione Kaggle → samples_all
        - Aggiunge SOLO “organico” (o tutte tranne “metal”, se configurato)
    Cella 3M: Split + Dataset + DataLoader
        - Divide 80/10/10 e definisce trasformazioni coerenti con ResNet18
    Cella 3B (opzionale): WeightedRandomSampler
        - Bilancia il training in caso di classi sbilanciate
    Cella 4: Modello + training loop
        - ResNet18 con fc a 5 classi; early stopping su val_loss; ReduceLROnPlateau
    Cella 5: Fine-tuning leggero (solo fc)
        - Congela tutto tranne la fc; label smoothing
        - Salva best_model_finetuned_light.pth (compatibile con l’app)
    Cella 6B: Calibrazione (Temperature Scaling) su validation
        - Stima T (grid search) e salva calibration.json
    Cella 7: Packaging artefatti
        - Crea cartella versionata + zip (pth, calibration.json, class_index.json)

Note / avvertenze:
    - HEIC/HEIF: in Colab spesso non c’è decoder; l’export dell’app è già in JPG/PNG → ok.
    - Out of memory: abbassa batch_size (Cella 3M).
    - Pochi dati: sotto ~50 campioni la qualità è bassa; raccogli più immagini + feedback.
    - Riproducibilità: stesso SEED = split stabili; piccole differenze di training sono normali.
    - Cambi classi: devi aggiornare sia il notebook sia l’app (CLASS_NAMES e template).

Mappatura Colab → App:
    - Copia in root del progetto Flask:
        • best_model_finetuned_light.pth
        • calibration.json
    - Non serve altro per far girare l’inferenza nell’app.

FAQ veloci:
    - Posso aggiungere solo “organico” da Kaggle? Sì (2K-SETUP + 2K-MERGE).
    - Perché la calibrazione? Rende le probabilità più “oneste” (meno over-confident).
    - Posso usare solo CPU? Sì, è più lento ma funziona.
